{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30bca05",
   "metadata": {},
   "source": [
    "# Spark & Databricks - Practice Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46067183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete hands-on examples for interview preparation\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Install required packages (run once)\n",
    "# !pip install pyspark findspark delta-spark\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import random\n",
    "\n",
    "# Create Spark Session with Delta Lake support\n",
    "\n",
    "spark = SparkSession.builder   \n",
    ".appName(“Spark Practice”)   \n",
    ".config(“spark.sql.extensions”, “io.delta.sql.DeltaSparkSessionExtension”)   \n",
    ".config(“spark.sql.catalog.spark_catalog”, “org.apache.spark.sql.delta.catalog.DeltaCatalog”)   \n",
    ".config(“spark.sql.adaptive.enabled”, “true”)   \n",
    ".master(“local[*]”)   \n",
    ".getOrCreate()\n",
    "\n",
    "print(f”Spark version: {spark.version}”)\n",
    "print(f”Spark UI: http://localhost:4040”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: SPARK FUNDAMENTALS\n",
    "# =============================================================================\n",
    "\n",
    "# Create sample data\n",
    "\n",
    "data = [\n",
    "    (1, “Alice”, 28, “US”, 50000),\n",
    "    (2, “Bob”, 35, “UK”, 60000),\n",
    "    (3, “Carol”, 32, “US”, 55000),\n",
    "    (4, “David”, 45, “DE”, 70000),\n",
    "    (5, “Eve”, 29, “UK”, 52000)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "StructField(“id”, IntegerType(), False),\n",
    "StructField(“name”, StringType(), False),\n",
    "StructField(“age”, IntegerType(), False),\n",
    "StructField(“country”, StringType(), False),\n",
    "StructField(“salary”, IntegerType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "# Basic operations\n",
    "\n",
    "print(”=== Basic Operations ===”)\n",
    "df.select(“name”, “salary”).show()\n",
    "df.filter(col(“age”) > 30).show()\n",
    "df.groupBy(“country”).agg(avg(“salary”).alias(“avg_salary”)).show()\n",
    "\n",
    "# Check partitions\n",
    "\n",
    "print(f”Number of partitions: {df.rdd.getNumPartitions()}”)\n",
    "\n",
    "# Repartition vs Coalesce\n",
    "\n",
    "df_repartitioned = df.repartition(10)\n",
    "print(f”After repartition: {df_repartitioned.rdd.getNumPartitions()}”)\n",
    "\n",
    "df_coalesced = df_repartitioned.coalesce(2)\n",
    "print(f”After coalesce: {df_coalesced.rdd.getNumPartitions()}”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: DELTA LAKE OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(”\\n=== Delta Lake Operations ===”)\n",
    "\n",
    "# Create Delta table\n",
    "\n",
    "delta_path = “/tmp/delta/users”\n",
    "df.write.format(“delta”).mode(“overwrite”).save(delta_path)\n",
    "\n",
    "# Read Delta table\n",
    "\n",
    "delta_df = spark.read.format(“delta”).load(delta_path)\n",
    "delta_df.show()\n",
    "\n",
    "# Time Travel - Version based\n",
    "\n",
    "df.write.format(“delta”).mode(“append”).save(delta_path)\n",
    "version_0 = spark.read.format(“delta”).option(“versionAsOf”, 0).load(delta_path)\n",
    "print(“Version 0 (original 5 rows):”)\n",
    "version_0.show()\n",
    "\n",
    "# MERGE (Upsert)\n",
    "\n",
    "updates = spark.createDataFrame([\n",
    "(1, “Alice”, 29, “US”, 52000),  # Update\n",
    "(6, “Frank”, 40, “FR”, 65000)   # Insert\n",
    "], schema)\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "deltaTable.alias(“target”).merge(\n",
    "updates.alias(“source”),\n",
    "“target.id = source.id”\n",
    ").whenMatchedUpdateAll()   \n",
    ".whenNotMatchedInsertAll()   \n",
    ".execute()\n",
    "\n",
    "print(“After MERGE:”)\n",
    "spark.read.format(“delta”).load(delta_path).show()\n",
    "\n",
    "# OPTIMIZE\n",
    "\n",
    "spark.sql(f”OPTIMIZE delta.`{delta_path}`”)\n",
    "\n",
    "# DESCRIBE HISTORY\n",
    "\n",
    "spark.sql(f”DESCRIBE HISTORY delta.`{delta_path}`”).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 4: DATA SKEW & SALTING\n",
    "# =============================================================================\n",
    "\n",
    "print(”\\n=== Data Skew & Salting ===”)\n",
    "\n",
    "# Create skewed dataset\n",
    "\n",
    "skewed_data = []\n",
    "\n",
    "# Bot user with 1000 events\n",
    "\n",
    "for i in range(1000):\n",
    "skewed_data.append((“bot”, f”event_{i}”, random.randint(1, 100)))\n",
    "\n",
    "# Normal users with 5 events each\n",
    "\n",
    "for user in [“alice”, “bob”, “carol”, “david”]:\n",
    "for i in range(5):\n",
    "skewed_data.append((user, f”event_{i}”, random.randint(1, 100)))\n",
    "\n",
    "skewed_df = spark.createDataFrame(\n",
    "skewed_data,\n",
    "[“user_id”, “event”, “value”]\n",
    ")\n",
    "\n",
    "# Check distribution\n",
    "\n",
    "print(“Key distribution (shows skew):”)\n",
    "skewed_df.groupBy(“user_id”).count().orderBy(desc(“count”)).show()\n",
    "\n",
    "# Apply salting\n",
    "\n",
    "salt_range = 10\n",
    "salted_df = skewed_df.withColumn(\n",
    "“salted_key”,\n",
    "when(col(“user_id”) == “bot”,\n",
    "concat(col(“user_id”), lit(”_”), floor(rand() * salt_range).cast(“int”)))\n",
    ".otherwise(col(“user_id”))\n",
    ")\n",
    "\n",
    "print(“After salting (bot split into 10 keys):”)\n",
    "salted_df.groupBy(“salted_key”).count().orderBy(desc(“count”)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032585b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 5: BROADCAST JOIN\n",
    "# =============================================================================\n",
    "\n",
    "print(”\\n=== Broadcast Join ===”)\n",
    "\n",
    "# Large table\n",
    "\n",
    "large_data = [(i, f”event_{i}”, random.choice([1, 2, 3, 4, 5]))\n",
    "for i in range(10000)]\n",
    "large_df = spark.createDataFrame(large_data, [“event_id”, “event_name”, “user_id”])\n",
    "\n",
    "# Small table\n",
    "\n",
    "small_data = [(1, “Alice”), (2, “Bob”), (3, “Carol”), (4, “David”), (5, “Eve”)]\n",
    "small_df = spark.createDataFrame(small_data, [“user_id”, “user_name”])\n",
    "\n",
    "# Regular join (both sides shuffle)\n",
    "\n",
    "regular_join = large_df.join(small_df, “user_id”)\n",
    "\n",
    "# Broadcast join (only large table processed)\n",
    "\n",
    "broadcast_join = large_df.join(broadcast(small_df), “user_id”)\n",
    "\n",
    "print(“Check query plans to see difference:”)\n",
    "print(”\\nRegular join plan:”)\n",
    "regular_join.explain()\n",
    "print(”\\nBroadcast join plan (no Exchange on small table):”)\n",
    "broadcast_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef886be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6: Z-ORDER OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(”\\n=== Z-ORDER Optimization ===”)\n",
    "\n",
    "# Create larger dataset for Z-ORDER demo\n",
    "\n",
    "zorder_data = []\n",
    "for i in range(10000):\n",
    "zorder_data.append((\n",
    "random.randint(1, 100),  # user_id\n",
    "random.choice([“2024-01-01”, “2024-01-02”, “2024-01-03”]),  # date\n",
    "random.choice([“US”, “UK”, “DE”, “FR”]),  # country\n",
    "random.randint(1, 1000)  # amount\n",
    "))\n",
    "\n",
    "zorder_df = spark.createDataFrame(\n",
    "zorder_data,\n",
    "[“user_id”, “date”, “country”, “amount”]\n",
    ")\n",
    "\n",
    "# Write as Delta\n",
    "\n",
    "zorder_path = “/tmp/delta/events”\n",
    "zorder_df.write.format(“delta”).mode(“overwrite”).save(zorder_path)\n",
    "\n",
    "# Check file count before optimization\n",
    "\n",
    "print(“Before OPTIMIZE:”)\n",
    "spark.sql(f”DESCRIBE DETAIL delta.`{zorder_path}`”).select(“numFiles”).show()\n",
    "\n",
    "# OPTIMIZE with Z-ORDER\n",
    "\n",
    "spark.sql(f”OPTIMIZE delta.`{zorder_path}` ZORDER BY (user_id, date)”)\n",
    "\n",
    "print(“After OPTIMIZE + Z-ORDER:”)\n",
    "spark.sql(f”DESCRIBE DETAIL delta.`{zorder_path}`”).select(“numFiles”).show()\n",
    "\n",
    "# Query benefits from Z-ORDER (check in Spark UI)\n",
    "\n",
    "result = spark.read.format(“delta”).load(zorder_path)   \n",
    ".filter((col(“user_id”) == 42) & (col(“date”) == “2024-01-02”))   \n",
    ".count()\n",
    "print(f”Query result (data skipping enabled): {result}”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e75b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: DATA QUALITY CHECKS\n",
    "# =============================================================================\n",
    "\n",
    "print(”\\n=== Data Quality Checks ===”)\n",
    "\n",
    "class DataQualityChecker:\n",
    "def **init**(self, df):\n",
    "self.df = df\n",
    "self.results = []\n",
    "\n",
    "```\n",
    "def expect_column_values_not_null(self, column, threshold=1.0):\n",
    "    null_count = self.df.filter(col(column).isNull()).count()\n",
    "    total = self.df.count()\n",
    "    null_pct = (null_count / total) * 100\n",
    "    passed = null_pct <= (100 - threshold * 100)\n",
    "    \n",
    "    self.results.append({\n",
    "        'check': 'not_null',\n",
    "        'column': column,\n",
    "        'passed': passed,\n",
    "        'null_percentage': null_pct\n",
    "    })\n",
    "    return self\n",
    "\n",
    "def expect_column_values_in_set(self, column, valid_values):\n",
    "    invalid_count = self.df.filter(~col(column).isin(valid_values)).count()\n",
    "    passed = invalid_count == 0\n",
    "    \n",
    "    self.results.append({\n",
    "        'check': 'in_set',\n",
    "        'column': column,\n",
    "        'passed': passed,\n",
    "        'invalid_count': invalid_count\n",
    "    })\n",
    "    return self\n",
    "\n",
    "def expect_column_values_between(self, column, min_val, max_val):\n",
    "    out_of_range = self.df.filter(\n",
    "        (col(column) < min_val) | (col(column) > max_val)\n",
    "    ).count()\n",
    "    passed = out_of_range == 0\n",
    "    \n",
    "    self.results.append({\n",
    "        'check': 'between',\n",
    "        'column': column,\n",
    "        'passed': passed,\n",
    "        'out_of_range_count': out_of_range\n",
    "    })\n",
    "    return self\n",
    "\n",
    "def get_results(self):\n",
    "    return self.results\n",
    "\n",
    "def raise_on_failure(self):\n",
    "    failed = [r for r in self.results if not r['passed']]\n",
    "    if failed:\n",
    "        raise ValueError(f\"Quality checks failed: {failed}\")\n",
    "    return self\n",
    "```\n",
    "\n",
    "# Test data with quality issues\n",
    "\n",
    "test_data = [\n",
    "(1, “Alice”, 28, “US”),\n",
    "(2, “Bob”, 35, “UK”),\n",
    "(3, None, 32, “US”),      # Null name\n",
    "(4, “David”, 150, “XX”),  # Invalid age and country\n",
    "(5, “Eve”, 29, “UK”)\n",
    "]\n",
    "\n",
    "test_df = spark.createDataFrame(\n",
    "test_data,\n",
    "[“id”, “name”, “age”, “country”]\n",
    ")\n",
    "\n",
    "# Run quality checks\n",
    "\n",
    "checker = DataQualityChecker(test_df)\n",
    "checker   \n",
    ".expect_column_values_not_null(“name”, threshold=1.0)   \n",
    ".expect_column_values_in_set(“country”, [“US”, “UK”, “DE”, “FR”])   \n",
    ".expect_column_values_between(“age”, 0, 120)\n",
    "\n",
    "print(“Quality check results:”)\n",
    "for result in checker.get_results():\n",
    "status = “✓ PASS” if result[‘passed’] else “✗ FAIL”\n",
    "print(f”{status}: {result[‘check’]} on {result[‘column’]}”)\n",
    "\n",
    "# Quarantine pattern\n",
    "\n",
    "def validate_with_quarantine(df):\n",
    "valid_condition = (\n",
    "col(“name”).isNotNull() &\n",
    "col(“age”).between(0, 120) &\n",
    "col(“country”).isin([“US”, “UK”, “DE”, “FR”])\n",
    ")\n",
    "\n",
    "```\n",
    "valid_df = df.filter(valid_condition)\n",
    "invalid_df = df.filter(~valid_condition).withColumn(\n",
    "    \"rejection_reason\", lit(\"Quality check failed\")\n",
    ")\n",
    "\n",
    "return valid_df, invalid_df\n",
    "```\n",
    "\n",
    "valid_data, invalid_data = validate_with_quarantine(test_df)\n",
    "\n",
    "print(”\\nValid records:”)\n",
    "valid_data.show()\n",
    "\n",
    "print(“Quarantined records:”)\n",
    "invalid_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 8: PERFORMANCE DIAGNOSTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(”\\n=== Performance Diagnostics ===”)\n",
    "\n",
    "# Create large dataset for performance testing\n",
    "\n",
    "perf_data = []\n",
    "for i in range(100000):\n",
    "perf_data.append((\n",
    "random.randint(1, 1000),\n",
    "f”user_{random.randint(1, 10000)}”,\n",
    "random.randint(1, 100)\n",
    "))\n",
    "\n",
    "perf_df = spark.createDataFrame(perf_data, [“id”, “user”, “value”])\n",
    "\n",
    "# Check partition distribution\n",
    "\n",
    "print(“Partition distribution:”)\n",
    "perf_df.groupBy(spark_partition_id()).count().show()\n",
    "\n",
    "# Analyze query plan\n",
    "\n",
    "print(”\\nQuery plan (look for shuffles - ‘Exchange’):”)\n",
    "perf_df.groupBy(“user”).agg(sum(“value”)).explain()\n",
    "\n",
    "# Cache frequently accessed data\n",
    "\n",
    "print(”\\nCaching demo:”)\n",
    "cached_df = perf_df.cache()\n",
    "cached_df.count()  # Materialize cache\n",
    "print(“Data cached - subsequent operations will be faster”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6152e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 9: MEDALLION ARCHITECTURE EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "print(”\\n=== Medallion Architecture ===”)\n",
    "\n",
    "# Bronze: Raw data\n",
    "\n",
    "bronze_data = [\n",
    "{“user_id”: “1”, “event”: “click”, “timestamp”: “2024-01-01T10:00:00”, “value”: “100”},\n",
    "{“user_id”: “2”, “event”: “purchase”, “timestamp”: “2024-01-01T11:00:00”, “value”: “200”},\n",
    "{“user_id”: None, “event”: “click”, “timestamp”: None, “value”: “50”},  # Bad data\n",
    "{“user_id”: “3”, “event”: “view”, “timestamp”: “2024-01-01T12:00:00”, “value”: “invalid”}\n",
    "]\n",
    "\n",
    "bronze_df = spark.createDataFrame(bronze_data)\n",
    "bronze_df.write.format(“delta”).mode(“overwrite”).save(”/tmp/delta/bronze_events”)\n",
    "print(“Bronze layer (raw data with issues):”)\n",
    "bronze_df.show()\n",
    "\n",
    "# Silver: Cleaned data\n",
    "\n",
    "silver_df = spark.read.format(“delta”).load(”/tmp/delta/bronze_events”)   \n",
    ".filter(col(“user_id”).isNotNull())   \n",
    ".filter(col(“timestamp”).isNotNull())   \n",
    ".withColumn(“user_id”, col(“user_id”).cast(“int”))   \n",
    ".withColumn(“value”, col(“value”).cast(“int”))   \n",
    ".filter(col(“value”).isNotNull())   \n",
    ".withColumn(“date”, to_date(col(“timestamp”)))\n",
    "\n",
    "silver_df.write.format(“delta”).mode(“overwrite”).save(”/tmp/delta/silver_events”)\n",
    "print(“Silver layer (cleaned and typed):”)\n",
    "silver_df.show()\n",
    "\n",
    "# Gold: Business aggregations\n",
    "\n",
    "gold_df = spark.read.format(“delta”).load(”/tmp/delta/silver_events”)   \n",
    ".groupBy(“date”, “event”)   \n",
    ".agg(\n",
    "count(”*”).alias(“event_count”),\n",
    "sum(“value”).alias(“total_value”)\n",
    ")\n",
    "\n",
    "gold_df.write.format(“delta”).mode(“overwrite”).save(”/tmp/delta/gold_daily_metrics”)\n",
    "print(“Gold layer (business metrics):”)\n",
    "gold_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 10: EXECUTOR CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(”\\n=== Executor Configuration ===”)\n",
    "\n",
    "# Check current configuration\n",
    "\n",
    "print(“Current Spark configuration:”)\n",
    "conf = spark.sparkContext.getConf().getAll()\n",
    "for key, value in conf:\n",
    "if ‘executor’ in key.lower() or ‘driver’ in key.lower():\n",
    "print(f”{key}: {value}”)\n",
    "\n",
    "# Example: Reconfigure (for demonstration)\n",
    "\n",
    "# In production, set these when creating SparkSession\n",
    "\n",
    "spark.conf.set(“spark.sql.shuffle.partitions”, “100”)  # Default is 200\n",
    "print(”\\nShuffle partitions set to 100 (reduces overhead for small data)”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8817b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 11: STREAMING EXAMPLE (Conceptual)\n",
    "# =============================================================================\n",
    "\n",
    "print(”\\n=== Streaming Example (Conceptual) ===”)\n",
    "\n",
    "# Note: This is a batch simulation of streaming\n",
    "\n",
    "# In production, use readStream with Kafka, Delta, etc.\n",
    "\n",
    "print(”””\n",
    "Streaming pattern in production:\n",
    "\n",
    "stream = spark.readStream \\\n",
    ".format(“delta”) \\\n",
    ".load(”/bronze/events”)\n",
    "\n",
    "query = stream.writeStream \\\n",
    ".format(“delta”) \\\n",
    ".outputMode(“append”) \\\n",
    ".option(“checkpointLocation”, “/checkpoints/stream”) \\\n",
    ".start(”/silver/events”)\n",
    "\n",
    "Key concepts:\n",
    "\n",
    "- readStream: Continuous data ingestion\n",
    "- writeStream: Continuous output\n",
    "- checkpointLocation: Fault tolerance\n",
    "- outputMode: append/complete/update\n",
    "  “””)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 12: CLEANUP\n",
    "# =============================================================================\n",
    "\n",
    "print(”\\n=== Cleanup ===”)\n",
    "\n",
    "# Uncomment to clean up Delta tables\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# shutil.rmtree(”/tmp/delta”, ignore_errors=True)\n",
    "\n",
    "# print(“Delta tables cleaned up”)\n",
    "\n",
    "# Stop Spark session\n",
    "\n",
    "# spark.stop()\n",
    "\n",
    "# print(“Spark session stopped”)\n",
    "\n",
    "print(”\\n=== Practice Complete! ===”)\n",
    "print(“Next steps:”)\n",
    "print(“1. Review Spark UI at http://localhost:4040”)\n",
    "print(“2. Experiment with different configurations”)\n",
    "print(“3. Try on larger datasets”)\n",
    "print(“4. Practice on Databricks Community Edition”)\n",
    "print(“5. Review the README for interview questions”)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
